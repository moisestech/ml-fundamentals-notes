# Regressions

## **1. Continuous Output Quiz**

ğŸ¥ [Udacity, Video Link](https://youtu.be/udJvijJvs1M)

---

## **2. Continuous Quiz**

ğŸ¥ [Udacity, Video Link](https://youtu.be/Bp6oBbLw8qE)

---

## **3. Age: Continuous or Discrete?**

ğŸ¥ [Udacity, Video Link](https://youtu.be/OrPlWwv19Jc)

---

## **4. Weather: Continuous or Discrete**

ğŸ¥ [Udacity, Video Link](https://youtu.be/jTKkq6DdJMw)

---

## **5. Email Author: Continuous or Discrete**

ğŸ¥ [Udacity, Video Link](https://youtu.be/GD9Bpjm31co)

---

## **6. Phone Number: Continuous or Discrete?**

ğŸ¥ [Udacity, Video Link](https://youtu.be/5dt0N4XN-y4)

---

## **7. Income: Continuous or Discrete?**

ğŸ¥ [Udacity, Video Link](https://youtu.be/4yapJV56YoM)

---

## **8. Continuous Feature Quiz**

ğŸ¥ [Udacity, Video Link](https://youtu.be/TIs9j-QITxw)

---

## **9. Supervised Learning w/ Continuous Output**

ğŸ¥ [Udacity, Video Link](https://youtu.be/dkLEMSLTxvk)

---

## **10. Equation of the Regression Line**

ğŸ¥ [Udacity, Video Link](https://youtu.be/--Pc1ASVjmM)

---

## **11. Slope and Intercept**

ğŸ¥ [Udacity, Video Link](https://youtu.be/Ksn1g5fCe1I)

---

## **12. Slope Quiz**

ğŸ¥ [Udacity, Video Link](https://youtu.be/Kn9v0KGDsvc)

---

## **13. Intercept Quiz**

ğŸ¥ [Udacity, Video Link](https://youtu.be/cuBxHYSPrkA)

---

## **14. Predictions Using Regression**

ğŸ¥ [Udacity, Video Link](https://youtu.be/cuBxHYSPrkA)

---

## **15. Adding An Intercept**

ğŸ¥ [Udacity, Video Link](https://youtu.be/xPDk70gKkjk)

---

## **16. Handoff to Katie**

ğŸ¥ [Udacity, Video Link](https://youtu.be/M3Nwl_B_bZ8)

---

## **17. Coding It Up**

ğŸ¥ [Udacity, Video Link](https://youtu.be/BTFOf2qXy5U)

---

## **18. Age/Net Worth Regression in sklearn**

ğŸ¥ [Udacity, Video Link](https://youtu.be/5DvUOwA7xhU)

---

## **19. Extracting Information from sklearn**

ğŸ¥ [Udacity, Video Link](https://youtu.be/zDIRQE_oxfk)

---

## **20. Extracting Score Data from sklearn**

ğŸ¥ [Udacity, Video Link](https://youtu.be/NhD4oUuhvO8)

---

## **21. Linear Regression Errors**

ğŸ¥ [Udacity, Video Link](https://youtu.be/A4nPMEOcUd4)

---

## **22. Error Quiz**

ğŸ¥ [Udacity, Video Link](https://youtu.be/PYNWtLgtRfU)

---

## **23. Errors and Fit Quality**

ğŸ¥ [Udacity, Video Link](https://youtu.be/PRQDaHphZhw)

---

## **24. Minimizing Sum of Squared Errors**

ğŸ¥ [Udacity, Video Link](https://youtu.be/E1XzT619Eug)

---

## **25. Algorithms for Minimizing Squared Errors**

ğŸ¥ [Udacity, Video Link](https://youtu.be/Dw_9Dp6wcJ8)

---

## **26. Why Minimize SSE**

ğŸ¥ [Udacity, Video Link](https://youtu.be/c1gvsNx_ypg)

---

## **27. Problem with Minimizing Absolute Errors**

ğŸ¥ [Udacity, Video Link](https://youtu.be/U46D7oEijlI)

---

## **28. Evaluating Regression by Eye**

ğŸ¥ [Udacity, Video Link](https://youtu.be/hS7cpq-sOeQ)

---

## **29. Problem with SSE**

ğŸ¥ [Udacity, Video Link](https://youtu.be/VD14oP-Ue6M)

---

## **30. R Squared Metric for Regression**

ğŸ¥ [Udacity, Video Link](https://youtu.be/yDJEP-XSWdU)

Note that R2 is only bounded from below by 0 when evaluating a linear regression on its training set. If evaluated on a significantly different set, where the predictions of a regressor are worse than simply guessing the mean value for the whole set, the calculation of R2 can be negative.

---

## **31. R Squared in SKlearn**

ğŸ¥ [Udacity, Video Link](https://youtu.be/Dxf1I4IE6co)

---

## **32. Visualizing Regression**

ğŸ¥ [Udacity, Video Link](https://youtu.be/zQAHZhcsXoQ)

---

## **33. What Data Is Good For Linear Regression**

ğŸ¥ [Udacity, Video Link](https://youtu.be/KFVdS328iC8)

As an advanced note, for the parabola shape in the lower-right, it is possible to fit non-linear relationships through use of feature transformations. For instance adding the squared x term as a feature gets us polynomial regression, which can be considered a special case of multiple linear regression and will be covered later in the lesson.

---

## **34. Comparing Classification and Regression**

ğŸ¥ [Udacity, Video Link](https://youtu.be/G_0W912qmGc)

---

## **35. Multivariate Regression Quiz**

ğŸ¥ [Udacity, Video Link](https://youtu.be/YPPQy_eB2mU)

---

## **36. Multi-Variate Regression Quiz 2**

ğŸ¥ [Udacity, Video Link](https://youtu.be/97v0kEWjcmg)

---

## **37. Regression Mini-Project Video**

ğŸ¥ [Udacity, Video Link](https://youtu.be/CrD9jN3rBM8)

---

## **38. Regression Mini-Project**

In this project, you will use regression to predict financial data for Enron employees and associates. Once you know some financial data about an employee, like their salary, what would you predict for the size of their bonus?

Get the data from [here](https://github.com/udacity/ud120-projects/tree/master/regression)

---

## **39. Bonus Target and Features**

Run the starter code found in regression/finance_regression.py. This will draw a scatterplot, with all the data points drawn in. What target are you trying to predict? What is the input feature being used to predict it?

Mentally (or better yet, print out the scatterplot and use paper and pencil) sketch out the regression line that you roughly predict.

Start Quiz

---

## **40. Visualizing Regression Data**

- In regression, you need training and testing data, just like in classification.

  - This has already been set up in the starter code.
  - Change the value of test_color from "b" to â€œrâ€ (for â€œredâ€), and rerun.
  - Note: For those students converting Python 2 code to Python 3, see below for some important remarks regarding compatibility.

- You will be fitting your regression using the blue (training) points only.

  - (You may have noticed that instead of the standard 10%, weâ€™ve put 50% of our data into the test set--thatâ€™s because in Part 5, we will switch the training and testing datasets around and splitting the data evenly makes this more straightforward.)

- From Python 3.3 forward, a change to the order in which dictionary keys are processed was made such that the orders are randomized each time the code is run.
  - This will cause some compatibility problems with the graders and project code, which were run under Python 2.7.
  - To correct for this, add the following argument to the featureFormat call on line 26 of `finance_regression.py`:

```python
sort_keys = '../tools/python2_lesson06_keys.pkl'
```

- This will open up a file in the tools folder with the Python 2 key order.

---

## **41. Extracting Slope and Intercept**

Import LinearRegression from sklearn, and create/fit your regression. Name it reg so that the plotting code will show it overlaid on the scatterplot. Does it fall approximately where you expected it?

Extract the slope (stored in the reg.coef\_ attribute) and the intercept. What are the slope and intercept?

Start Quiz
Note: If you are not getting the results expected by the grader, then you may want to check the file tools/feature_format.py. Due to changes in the final project, some file changes have affected the numbers output on this assignment as written. Check that you have the most recent version of the file from the repository, such that the featureFormat has a default parameter for sort_keys = False and that keys = dictionary.keys() results. There should also be a test_list variable that is used to filter missing data.

---

## **42. Regression Score: Training Data**

Imagine you were a less savvy machine learner, and didnâ€™t know to test on a holdout test set. Instead, you tested on the same data that you used to train, by comparing the regression predictions to the target values (i.e. bonuses) in the training data. What score do you find? You may not have an intuition yet for what a â€œgoodâ€ score is; this score isnâ€™t very good (but it could be a lot worse).

Start Quiz

---

## **43. Regression Score: Test Data**

Now compute the score for your regression on the test data, like you know you should. Whatâ€™s that score on the testing data? If you made the mistake of only assessing on the training data, would you overestimate or underestimate the performance of your regression?

Start Quiz

---

## **44. Regressing Bonus Against LTI**

There are lots of finance features available, some of which might be more powerful than others in terms of predicting a personâ€™s bonus. For example, suppose you thought about the data a bit and guess that the â€œlong_term_incentiveâ€ feature, which is supposed to reward employees for contributing to the long-term health of the company, might be more closely related to a personâ€™s bonus than their salary is.

A way to confirm that youâ€™re right in this hypothesis is to regress the bonus against the long term incentive, and see if the regression score is significantly higher than regressing the bonus against the salary. Perform the regression of bonus against long term incentive--whatâ€™s the score on the test data?

Start Quiz

---

## **45. Salary vs LTI for Predicting Bonus**

If you had to predict someoneâ€™s bonus and you could only have one piece of information about them, would you rather know their salary or the long term incentive that they received?

Start Quiz

---

## **46. Sneak Peek: Outliers Break Regressions**

This is a sneak peek of the next lesson, on outlier identification and removal. Go back to a setup where you are using the salary to predict the bonus, and rerun the code to remind yourself of what the data look like. You might notice a few data points that fall outside the main trend, someone who gets a high salary (over a million dollars!) but a relatively small bonus. This is an example of an outlier, and weâ€™ll spend lots of time on them in the next lesson.

A point like this can have a big effect on a regression: if it falls in the training set, it can have a significant effect on the slope/intercept if it falls in the test set, it can make the score much lower than it would otherwise be As things stand right now, this point falls into the test set (and probably hurting the score on our test data as a result). Letâ€™s add a little hack to see what happens if it falls in the training set instead. Add these two lines near the bottom of finance_regression.py, right before plt.xlabel(features_list[1]):

reg.fit(feature_test, target_test)
plt.plot(feature_train, reg.predict(feature_train), color="b")

Now weâ€™ll be drawing two regression lines, one fit on the test data (with outlier) and one fit on the training data (no outlier). Look at the plot now--big difference, huh? That single outlier is driving most of the difference. Whatâ€™s the slope of the new regression line?

(Thatâ€™s a big difference, and itâ€™s mostly driven by the outliers. The next lesson will dig into outliers in more detail so you have tools to detect and deal with them.)

Start Quiz

---

## Foam Related Links

- **[[ml-algo]]**
