# Naive Bayes

## [ğŸ“ Naive Bayes, Lesson 2, Udacity, UD120](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/29561391780923)

---

## **1. ML in The Google Self-Driving Car**

ğŸ¥ [Udacity, Video Link](https://youtu.be/aALYYSwS7MM)

- In this video we're discussing the self-driving car and the technology behind land section.

---

## **2. Acerous Vs Non-Acerous**

ğŸ¥ [Udacity, Video Link](https://youtu.be/TeFF9wXiFfs)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30775389770923#)

---

## **3. Supervised Classification Example**

ğŸ¥ [Udacity, Video Link](https://youtu.be/buxApBhZCO0)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/23458286770923#)

---

## **4. Features and Labels Musical Example**

ğŸ¥ [Udacity, Video Link](https://youtu.be/rnv0-lG9yKU)

---

## **5. Features Visualization Quiz**

ğŸ¥ [Udacity, Video Link](https://youtu.be/t0iflCpBUDA)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/23834887130923#)

---

## **6. Classification By Eye**

ğŸ¥ [Udacity, Video Link](https://youtu.be/xeMDpSRTLWc)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/24177185370923#)

---

## **7. Intro To Stanley Terrain Classification**

ğŸ¥ [Udacity, Video Link](https://youtu.be/TwFhCeov85E)

---

## **8. Speed Scatterplot: Grade and Bumpiness**

ğŸ¥ [Udacity, Video Link](https://youtu.be/IMWsjjIeOrY)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30162187150923#)

---

## **9. Speed Scatterplot 2**

ğŸ¥ [Udacity, Video Link](https://youtu.be/T4GbEVybNlY)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30324487400923#)

---

## **10. Speed Scatterplot 3**

ğŸ¥ [Udacity, Video Link](https://youtu.be/PaE5caOJ5kg)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30128387240923#)

---

## **11. From Scatterplots to Predictions**

ğŸ¥ [Udacity, Video Link](https://youtu.be/dGS0SKu1ox0)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30140587780923#)

---

## **12. From Scatterplots to Predictions 2**

ğŸ¥ [Udacity, Video Link](https://youtu.be/tkllhaHoko8)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30152286580923#)

---

## **13. From Scatterplots to Decision Surfaces**

ğŸ¥ [Udacity, Video Link](https://youtu.be/gbkORDbJM50)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30047786360923#)

---

## **14. A Good Linear Decision Surface**

ğŸ¥ [Udacity, Video Link](https://youtu.be/sudTOiG-NJo)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30031787130923#)

---

## **15. Transition to Using Naive Bayes**

ğŸ¥ [Udacity, Video Link](https://youtu.be/2_dJXh1qqe0)

---

## **16. NB Decision Boundary in Python**

ğŸ¥ [Udacity, Video Link](https://youtu.be/pauohSxuCVs)

---

## **17. Getting Started With sklearn**

ğŸ¥ [Udacity, Video Link](https://youtu.be/olGPVtH7KGU)

---

## **18. Gaussian NB Example**

ğŸ¥ [Udacity, Video Link](https://youtu.be/wpnDwiqTCJA)

---

## **19. GaussianNB Deployment on Terrain Data**

ğŸ¥ [Udacity, Video Link](https://youtu.be/VBs6D4ggnYY)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/24196685390923#)

- To find the `ClassifyNB.py` script that you need to update for the quiz, you can click on the dropdown in the classroom code editor to get a list of files that will be used.

- In the quiz that follows, the line that reads
  `pred = clf.predict(features_test)`
  is not necessary for drawing the decision boundary, at least as we've written the code.

- However, the whole point of making a classifier is that you can make predictions with it, so be sure to keep it in mind since you'll be using it in the quiz after this one.

---

## **20. Calculating NB Accuracy**

ğŸ¥ [Udacity, Video Link](https://youtu.be/-gJJmckPBAg)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/22853385400923#)

- In this example, we print the accuracy. However, the quiz is slightly different in that you return the accuracy, rather than printing it directly.

- Remember, accuracy is defined as the number of test points that are classified correctly divided by the total number of test points.

- There's another way you can do this, too â€“

- `print clf.score(features_test, labels_test)`

---

## **21. Training and Testing Data**

ğŸ¥ [Udacity, Video Link](https://youtu.be/x2dmBUEKQIA)

---

## **22. Unpacking NB Using Bayes Rule**

ğŸ¥ [Udacity, Video Link](https://youtu.be/lGREq530kfU)

---

## **23. Bayes Rule**

ğŸ¥ [Udacity, Video Link](https://youtu.be/1biLtViOQDc)

---

## **24. Cancer Test**

ğŸ¥ [Udacity, Video Link](https://youtu.be/EL5z2lUuxY4)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/29897086260923#)

- Reminder of the question: the prior probability of cancer is 1%, and a sensitivity and specificity are 90%, what's the probability that someone with a positive cancer test actually has the disease?

---

## **25. Prior and Posterior**

ğŸ¥ [Udacity, Video Link](https://youtu.be/EL5z2lUuxY4)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30134285350923#)

---

## **26. Normalizing 1**

ğŸ¥ [Udacity, Video Link](https://youtu.be/aALYYSwS7MM)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/29561391780923#)

---

## **27. Normalizing 2**

ğŸ¥ [Udacity, Video Link](https://youtu.be/EPrrQaYp7H0)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30061885360923#)

---

## **28. Normalizing 3**

ğŸ¥ [Udacity, Video Link](https://youtu.be/FBRK-XwPC54)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30081285350923#)

---

## **29. Total Probability**

ğŸ¥ [Udacity, Video Link](https://youtu.be/FlbDcNPGgUE)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/29913289200923#)

---

## **30. Bayes Rule Diagram**

ğŸ¥ [Udacity, Video Link](https://youtu.be/LIQrs3dviIs)

---

## **31. Bayes Rule for Classification**

ğŸ¥ [Udacity, Video Link](https://youtu.be/wPI9WOfpZbM)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/23503786520923#)
Note that 50% (.5) of Sara's words are love.

---

## **32. Chris or Sara**

ğŸ¥ [Udacity, Video Link](https://youtu.be/nNna_SLlIT8)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/24322585350923#)
Note that 50% (.5) of Sara's words are love.

---

## **33. Posterior Probabilities**

ğŸ¥ [Udacity, Video Link](https://youtu.be/lJlS-Xdlu5o)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30226787120923#)
Note that 50% (.5) of Sara's words are love.

---

## **34. Bayesian Probabilities On Your Own**

ğŸ¥ [Udacity, Video Link](https://youtu.be/2StCBxTOoK0)

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/24144385410923#)
Note that 50% (.5) of Sara's words are love.

---

## **35. Why Is Naive Bayes Naive**

ğŸ¥ [Udacity, Video Link](https://youtu.be/H7IlFC5wbjk)
ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/24261685360923#)

---

## **36. Naive Bayes Strengths and Weaknesses**

ğŸ¥ [Udacity, Video Link](https://youtu.be/nfbKTrufPOs)

ğŸ“ [**Start Quiz**]()

---

## **37. Congrats on Learning Naive Bayes**

ğŸ¥ [Udacity, Video Link](https://youtu.be/nQsYfzO7-00)

---

## **38. Naive Bayes Mini-Project Video**

ğŸ¥ [Udacity, Video Link](https://youtu.be/UH2oSijkszo)

---

## **39. Getting Started with Mini-Projects**

- Welcome to the first mini-project! Here's what to expect:

- you'll download the project starter files (one time only)
- you'll run a startup script
- near the end of each lesson, there will be a video and/or reading node telling you about the mini-project for that lesson
- then, a series of reading nodes at the end of the lesson will walk you through what you should do for that mini-project, and give you a chance to enter the answers that you get as you work through the tasks (these will look like quizzes)
- you'll develop the code on your own computer based on the instructions in the reading nodes
- the code you write will enable you to answer the quiz questions

- So, what now?

- if you know git, you can clone the starter files: git clone [`https://github.com/udacity/ud120-projects.git`](https://github.com/udacity/ud120-projects.git)

- If you don't know git, Udacity has a great (and short) course that can get you started

---

## **40. Machine Learning for Author ID**

- A couple of years ago, J.K. Rowling (of Harry Potter fame) tried something interesting.

  - She wrote a book, â€œThe Cuckooâ€™s Calling,â€ under the name Robert Galbraith.
  - The book received some good reviews, but no one paid much attention to it--until an anonymous tipster on Twitter said it was J.K. Rowling.
  - The London Sunday Times enlisted two experts to compare the linguistic patterns of â€œCuckooâ€ to Rowlingâ€™s â€œThe Casual Vacancy,â€ as well as to books by several other authors.
  - After the results of their analysis pointed strongly toward Rowling as the author, the Times directly asked the publisher if they were the same person, and the publisher confirmed. The book exploded in popularity overnight.

- Weâ€™ll do something very similar in this project.

  - We have a set of emails, half of which were written by one person and the other half by another person at the same company.
  - Our objective is to classify the emails as written by one person or the other based only on the text of the email.
  - We will start with Naive Bayes in this mini-project, and then expand in later projects to other algorithms.

- We will start by giving you a list of strings.

  - Each string is the text of an email, which has undergone some basic preprocessing; we will then provide the code to split the dataset into training and testing sets.
  - (In the next lessons youâ€™ll learn how to do this preprocessing and splitting yourself, but for now weâ€™ll give the code to you).

- One particular feature of Naive Bayes is that itâ€™s a good algorithm for working with text classification.
  - When dealing with text, itâ€™s very common to treat each unique word as a feature, and since the typical personâ€™s vocabulary is many thousands of words, this makes for a large number of features.
  - The relative simplicity of the algorithm and the independent features assumption of Naive Bayes make it a strong performer for classifying texts.
  - In this mini-project, you will download and install sklearn on your computer and use Naive Bayes to classify emails by author.

---

## **41. Getting Your Code Set Up**

- Check that you have a working python installation, preferably version 2.6 or 2.7 (thatâ€™s the version that we use--other versions may work, but we canâ€™t guarantee it).

  - If you're using Python 3, you may find some more extensive revision of code to get things working at this time, so you may want to set up a Python 2 environment to work through the course materials.

- We will use pip to install some packages.

  - First get and install pip from here.
  - Using pip, install a bunch of python packages:

- go to your terminal line (donâ€™t open python, just the command prompt)

- install sklearn: pip install scikit-learn
  -- for your reference, hereâ€™s the sklearn installation instructions

- install natural language toolkit: pip install nltk

- Get the Intro to Machine Learning source code. You will need git to clone the repository: git clone `https://github.com/udacity/ud120-projects.git`

- You only have to do this once, the code base contains the starter code for all the mini-projects.
  - Go into the `tools/` directory, and run `startup.py`.
  - It will first check for the python modules, then download and unzip a large dataset that we will use heavily later.
  - The `download/unzipping` can take a while, but you donâ€™t have to wait for it to finish in order to get started on Part 1.

---

## **42. Author ID Accuracy**

- Create and train a Naive Bayes classifier in `naive_bayes/nb_author_id.py`.

  - Use it to make predictions for the test set. What is the accuracy?

- When training you may see the following error: UserWarning: Duplicate scores.

  - Result may depend on feature ordering.
  - There are probably duplicate features, or you used a classification score for a regression task.
  - warn("Duplicate scores. Result may depend on feature ordering.")

- This is a warning that two or more words happen to have the same usage patterns in the emails--as far as the algorithm is concerned, this means that two features are the same.
  - Some algorithms will actually break (mathematically wonâ€™t work) or give multiple different answers (depending on feature ordering) when there are duplicate features and sklearn is giving us a warning.
  - Good information, but not something we have to worry about.

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30245185880923#)

- Some students have encountered memory problems when executing the code for this problem.
  - To reduce the chance of seeing a memory error while running the code, we recommend that you use a computer with at least 2GB of RAM.
  - If you find that the code is causing memory errors, you can also try setting `test_size = 0.5` in the email_preprocess.py file.

---

## **43. Timing Your NB Classifier**

- An important topic that we didnâ€™t explicitly talk about is **the time to train and test our algorithms**.
  - Put in two lines of code, above and below the line fitting your classifier, like this:

```python
t0 = time()
< your clf.fit() line of code >
print "training time:", round(time()-t0, 3), "s"
```

- Put similar lines of code around the `clf.predict()` line of code, so you can compare the time to train the classifier and to make predictions with it.

  - What is faster, training or prediction?

- **We will compare the Naive Bayes timing to a couple other algorithms**, so note down the speed and accuracy you get and weâ€™ll revisit this in the next mini-project.

ğŸ“ [**Start Quiz**](https://classroom.udacity.com/courses/ud120/lessons/2254358555/concepts/30127485730923#)

---

## Resources

- [Naive Bayes, Clearly Explained](https://youtu.be/O2L2Uv9pdDA), **YouTube**, _StatQuest with Josh Starmer_

---

## Foam Related Links

- [[_scatterplot]]
- [[_features]]
- [[_claasification]]

- **[[_supervised]]**
