# RNNs

## **1. Intro to RNNs**

Recurrent Neural Networks
Hi! It's Luis again!

Now that you have some experience with PyTorch and deep learning, I'll be teaching you about recurrent neural networks (RNNs) and long short-term memory (LSTM) . RNNs are designed specifically to learn from sequences of data by passing the hidden state from one step in the sequence to the next step in the sequence, combined with the input. LSTMs are an improvement the RNNs, and are quite useful when our neural network needs to switch between remembering recent things, and things from long time ago. But first, I want to give you some great references to study this further. There are many posts out there about LSTMs, here are a few of my favorites:

Chris Olah's LSTM post
Edwin Chen's LSTM post
Andrej Karpathy's blog post on RNNs
Andrej Karpathy's lecture on RNNs and LSTMs from CS231n
So, let's dig in!

---

## **2. RNN vs LSTM**

---

## **3. Basics of LSTM**

---

## **4. Architecture of LSTM**

---

## **5. The Learn Gate**

---

## **6. The Forget Gate**

---

## **7. The Remember Gate**

---

## **8. The Use Gate**

---

## **9. Putting it All Together**

---

## **10. Other architectures**

---

## **11. Implementing RNNs**

---

## **12. Time-Series Prediction**

---

## **13. Training & Memory**

---

## **14. Character-wise RNNs**

---

## **15. Sequence Batching**

---

## **16. Notebook: Character-Level RNN**

---

## **17. Implementing a Char-RNN**

---

## **18. Batching Data, Solution**

---

## **19. Defining the Model**

---

## **20. Char-RNN, Solution**

---

## **21. Making Predictions**
